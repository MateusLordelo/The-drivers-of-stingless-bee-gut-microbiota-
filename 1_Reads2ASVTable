###############################################
# dada2_runs_subset.R
# DADA2 pipeline for sequencing runs (subset)
# Usage:
# 1) Install cutadapt (see README / instructions)
# 2) Run in R: source("dada2_runs_subset.R")
#
# Author: (Matt)
###############################################

#------------------------- (0) install packages------------------------------
if (!requireNamespace("BiocManager", quietly = TRUE)) {
    install.packages("BiocManager")
}
auto_install <- function(pkgs){
   for(p in pkgs){
    # Check if the package is already installed
    if(!requireNamespace(p, quietly = TRUE)){
      # Use BiocManager for installation (it handles CRAN and Bioc packages)
      message(paste("Installing:", p))
      BiocManager::install(p, update = FALSE, ask = FALSE)
    } else {
      message(paste(p, "is already installed."))
    }
    
    # Load the package
    message(paste("Loading:", p))
    library(p, character.only = TRUE)
  }
}

# Install required packages
auto_install(c("dada2", "ggplot2", "vegan", "ShortRead", "DECIPHER", "phangorn", "Biostrings", "phyloseq", "dplyr", "ape"))

library(dada2)
library(ShortRead) # for read inspection (optional)
packageVersion("dada2")

# ---------- USER SETTINGS ----------
# Paths: relative to working directory (where you run this script)
name_analysis <- "Name"
path_run1 <- "PATH"     # contains files like unique-name_16S-V4V5_R1.fastq (or .fastq.gz)
silva_tax_file <- ("PATH")
silva_species_file <- ("PATH")
outdir <- paste0("dada2_out_", name_analysis)
dir.create(outdir, showWarnings = FALSE)

# DADA2 processing params (initial; change after QC)
truncLen_run1 <- c(0, 0)  # initial guess (forward, reverse)
maxEE_run1 <- c(2, 2)
truncQ <- 2
maxN <- 0
pooling <- FALSE              # set to TRUE or "pseudo" if desired
# Multithread
multithread <- TRUE

# ---------- FILE COLLECTION ----------
# Run1 raw files (pattern: *_R1.fastq or *_R1.fastq.gz)
fnFs_run1_all <- sort(list.files(path_run1, pattern="_1.fastq.gz", full.names = TRUE, recursive = TRUE))
fnRs_run1_all <- sort(list.files(path_run1, pattern="_2.fastq.gz", full.names = TRUE, recursive = TRUE))
if(length(fnFs_run1_all)==0) stop("No run1 files found in path: ", path_run1)
##exclude raw_
fnFs_run1_all <- fnFs_run1_all[!grepl("raw_", fnFs_run1_all)]
fnRs_run1_all <- fnRs_run1_all[!grepl("raw_", fnRs_run1_all)]

# Infer sample names by stripping suffix
sampnames_run1_all <- sapply(basename(fnFs_run1_all), function(x) sub("_1.fq$|_1.fastq.gz$", "", x))
# select subset
sel_run1 <- seq_len(length(fnFs_run1_all))
fnFs_run1 <- fnFs_run1_all[sel_run1]; fnRs_run1 <- fnRs_run1_all[sel_run1]
sample.names.run1 <- paste0(sampnames_run1_all[sel_run1])

# Print what will be processed
cat("Chosen run1 samples:\n"); print(sample.names.run1)

# ---------- 1) Cutadapt on run1 (paired) was done, skip ----------
cutFs_run1 <- fnFs_run1
cutRs_run1 <- fnRs_run1

# ---------- 2) QC plots (inspect) ----------
pdf(file.path(outdir, "QC_run1_cutadapt.pdf"))
plotQualityProfile(cutFs_run1[1:min(3, length(cutFs_run1))])
plotQualityProfile(cutRs_run1[1:min(3, length(cutRs_run1))])
dev.off()

cat("QC plots written to", outdir, "\n")
cat("Please inspect these PDFs and adjust truncLen values if needed. Continuing with defaults...\n")

# ---------- 3) filterAndTrim  skip----------
filt_path_run1 <- file.path(path_run1, "filtered")
dir.create(filt_path_run1, showWarnings = FALSE)
filtFs_run1 <- file.path(filt_path_run1, paste0(sample.names.run1, "_F_filt.fastq.gz"))
filtRs_run1 <- file.path(filt_path_run1, paste0(sample.names.run1, "_R_filt.fastq.gz"))

cat("Filtering run1 (cutadapt outputs) with filterAndTrim...\n")
out_filt_run1 <- filterAndTrim(cutFs_run1, filtFs_run1, cutRs_run1, filtRs_run1,
                              truncLen = truncLen_run1,
                              maxEE = maxEE_run1,
                              truncQ = truncQ,
                              maxN = maxN,
                              rm.phix = TRUE,
                              compress = TRUE,
                              multithread = multithread)

rownames(out_filt_run1) <- sample.names.run1
# ---------- 4) Learn errors per run ----------
cat("Learning errors for run1...\n")
errF_run1 <- learnErrors(filtFs_run1, multithread = multithread)
errR_run1 <- learnErrors(filtRs_run1, multithread = multithread)

# Optional: save error plots
pdf(file.path(outdir, "error_plots_run1.pdf"))
plotErrors(errF_run1, nominalQ = TRUE)
plotErrors(errR_run1, nominalQ = TRUE)
dev.off()

# ---------- 5) Derep and dada per run ----------
# Run1 paired
cat("Derep + dada run1...\n")
derepFs1 <- derepFastq(filtFs_run1); names(derepFs1) <- sample.names.run1
derepRs1 <- derepFastq(filtRs_run1); names(derepRs1) <- sample.names.run1
dadaFs1 <- dada(derepFs1, err = errF_run1, multithread = multithread, pool = pooling)
dadaRs1 <- dada(derepRs1, err = errR_run1, multithread = multithread, pool = pooling)
mergers1 <- mergePairs(dadaFs1, derepFs1, dadaRs1, derepRs1, verbose = TRUE)

# --- merge diagnostics (run1) ---
# mergers1 is a list of data.frames returned by mergePairs
merged_counts <- sapply(mergers1, function(m) {
  # m may be a data.frame with column 'abundance' or 'count'
  if(is.data.frame(m)) {
    if("abundance" %in% colnames(m)) return(sum(m$abundance))
    if("count" %in% colnames(m)) return(sum(m$count))
  }
  # fallback
  return(NA_integer_)
})

# filtered input counts from filterAndTrim output (out_filt_run1)
# out_filt_run1 rows correspond to samples in same order as filtFs_run1
filtered_in <- NULL
if(exists("out_filt_run1")) {
  # out_filt_run1[,1] is input reads, [,2] is reads after filtering
  filtered_in <- out_filt_run1[,2]  # reads retained after filtering (passed to dada/merge)
  names(filtered_in) <- rownames(out_filt_run1)
}

# match names: mergers1 names should be sample names
merged_df <- data.frame(
  Sample = names(merged_counts),
  Merged = as.integer(merged_counts),
  stringsAsFactors = FALSE
)

# attach filtered counts if available
if(!is.null(filtered_in)) {
  # attempt to align by sample name
  matched_filtered <- filtered_in[merged_df$Sample]
  merged_df$Filtered <- as.integer(matched_filtered)
  merged_df$PercentMerged = round(100 * merged_df$Merged / merged_df$Filtered, 1)
} else {
  merged_df$Filtered <- NA_integer_
  merged_df$PercentMerged <- NA_real_
}

# basic summary
print("Merge summary (first 20 rows):")
print(head(merged_df, 20))
cat("\nOverall merged/filtered (run1):\n")
if(!is.null(filtered_in)) {
  cat("Total filtered reads: ", sum(merged_df$Filtered, na.rm=TRUE), "\n")
  cat("Total merged reads:   ", sum(merged_df$Merged, na.rm=TRUE), "\n")
  cat("Percent merged:       ", round(100 * sum(merged_df$Merged, na.rm=TRUE) / sum(merged_df$Filtered, na.rm=TRUE), 1), "%\n")
}

# flag poor samples
poor <- merged_df$PercentMerged < 30 & !is.na(merged_df$PercentMerged)
if(any(poor)) {
  cat("\nWARNING: samples with PercentMerged < 30% (possible problem):\n")
  print(merged_df[poor, , drop=FALSE])
}

# plot distribution of percent merged
pdf(file.path(outdir, "QC_PercentMerged_run1.pdf"), width=6, height=5)
if(requireNamespace("ggplot2", quietly=TRUE)) {
library(ggplot2)
g <- ggplot(merged_df, aes(x=PercentMerged)) +
geom_histogram(binwidth=5, na.rm=TRUE) +
labs(title="Distribution of percent merged (run1)", x="% merged", y="n samples")
print(g) # print works inside the pdf() block
} else {
hist(merged_df$PercentMerged, breaks=seq(0,100,5), main="Percent merged (run1)", xlab="% merged", ylab="samples")
}
dev.off() # <--- Crucial to close the device

# also inspect merged-sequence-length distribution (before seqtab)
# concat all merged sequences in mergers1 to compute lengths (may be heavy)
merged_seqs <- unlist(lapply(mergers1, function(m) if(is.data.frame(m) && "sequence" %in% colnames(m)) m$sequence else NULL))
if(length(merged_seqs) > 0) {
  cat("\nMerged sequence length summary (run1):\n")
  print(summary(nchar(merged_seqs)))
  print(table(nchar(merged_seqs))[1:20]) # top lengths
} else {
  cat("No 'sequence' column found in mergers1 objects; length summary not available.\n")
}
# --- end diagnostics ---

# -------------- Make sequence table ----------------

seqtab1 <- makeSequenceTable(mergers1)

# ---------- 6) Merge runs and remove chimeras ----------
cat("Merging run1 and run2 sequence tables...\n")
# If only one run, seqtab_all is seqtab1
seqtab_all <- seqtab1

cat("Total dimensions of merged seqtab (samples x ASVs):\n")
print(dim(seqtab_all))

cat("Removing chimeras on merged table...\n")
seqtab_all_nochim <- removeBimeraDenovo(seqtab_all, method = "consensus", multithread = multithread, verbose = TRUE)

cat("Non-chimeric seqtab dimensions:\n"); print(dim(seqtab_all_nochim))

# Create diagnostics directory
diag_dir <- file.path(outdir, "diagnostics")
dir.create(diag_dir, showWarnings = FALSE)

# Build the definitive read tracking table (Corrected Logic)
tracking_auto <- data.frame(
Sample = sample.names.run1,
Input = out_filt_run1[,1],
Filtered = out_filt_run1[,2],
# seqtab1 rows are samples, representing Merged counts
Denoised = rowSums(seqtab1)[sample.names.run1], # Use merged counts for both
Merged = rowSums(seqtab1)[sample.names.run1],
# CORRECTED: Use rowSums for Nonchim counts per sample
Nonchim = rowSums(seqtab_all_nochim)[sample.names.run1], 
stringsAsFactors = FALSE
)

write.csv(tracking_auto,file.path(diag_dir, "read_tracking_auto.csv"), row.names = FALSE)

# Plots (Plot 2 now uses the corrected Nonchim column)
pdf(file.path(diag_dir, "diagnostic_plots.pdf"), width=8, height=6)
# Plot 1 — Merge success
hist(tracking_auto$Merged / tracking_auto$Filtered, main="Percent Merged Reads", xlab="Merged / Filtered")
# Plot 2 — Non-chimeric reads
hist(tracking_auto$Nonchim, main="Non-Chimeric Reads per Sample", xlab="Reads")
dev.off()

# Rarefaction Curve Plot
# Convert seqtab matrix to vegan-compatible object
otu <- (seqtab_all_nochim)
depth <- rowSums(otu)
richness <- vegan::specnumber(otu)
pdf(file.path(diag_dir,"rarefaction_depthcolored.pdf"))
rarecurve(
  otu, 
  step = 200, 
  label=FALSE,
  col = colorRampPalette(c("lightblue","darkblue"))(100)[rank(depth)]
)
dev.off()
pdf(file.path(diag_dir,"richness_vs_depth.pdf"))
plot(depth, richness, 
     pch=19, 
     xlab="Sequencing depth",
     ylab="Observed richness (ASVs)",
     main="Saturation diagnostic: Richness vs Depth")
abline(lm(richness ~ depth), col="red")
dev.off()

cat("\nDiagnostic PDF, tracking table, and rarefaction curves saved in:\n")
cat(diag_dir, "\n\n")

# ---------- 7) Save outputs (Simplified) ----------
# write ASV fasta (Sequences as headers)
asv_seqs <- colnames(seqtab_all_nochim)
asv_headers <- paste0(">ASV", seq_along(asv_seqs))
asv_fasta <- c(rbind(asv_headers, asv_seqs))
writeLines(asv_fasta, file.path(outdir, "ASVs.fa"))

# write count table (ASVs rows x samples columns)
asv_tab <- t(seqtab_all_nochim)
colnames(asv_tab) <- rownames(seqtab_all_nochim) # Set sample names
write.csv(asv_tab, file.path(outdir, "ASV_counts.csv"), quote = FALSE)

# Save seqtab object (The most important R object)
saveRDS(seqtab_all_nochim, file.path(outdir, "seqtab_all_nochim.rds"))

# Final outputs summary
cat("Outputs written to", outdir, "\n")
cat("ASV fasta: ", file.path(outdir, "ASVs.fa"), "\n")
cat("ASV counts: ", file.path(outdir, "ASV_counts.csv"), "\n")
cat("Read tracking: ", file.path(outdir, "read_tracking_auto.csv"), "\n")
cat("RDS seqtab: ", file.path(outdir, "seqtab_all_nochim.rds"), "\n")
cat("\nDone. Inspect QC plots and tracking table. Then check which samples/libraries lost many reads or need parameter tuning.\n")


##assign taxonomy

taxa <- assignTaxonomy(seqtab_all_nochim, silva_tax_file)
taxa <- addSpecies(taxa, silva_species_file)

# Inspect taxonomic assignment
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
# Write original taxonomy table to outdir
write.csv(taxa, file.path(outdir, "ASV_table_taxonomy_SEQ.csv"), quote = FALSE)
cat("Original taxonomy table saved to:", file.path(outdir, "ASV_table_taxonomy_SEQ.csv"), "\n")

# --- 2. Create the ASV Map Table ---
# The ASV sequences are the column names of the sequence table
ASV_sequences <- colnames(seqtab_all_nochim)
N_ASVs <- length(ASV_sequences)
# Create unique ASV IDs (ASV_1, ASV_2, ..., ASV_n)
ASV_IDs <- paste0("ASV_", seq_len(N_ASVs))
# Create the map file: Sequence | ASV_ID
ASV_map <- data.frame(
 Sequence = ASV_sequences,
 ASV_ID = ASV_IDs,
 stringsAsFactors = FALSE
)
# Write the ASV map to a file
write.csv(ASV_map, file.path(outdir, "ASV_sequence_to_ID_map.csv"), row.names = FALSE, quote = FALSE)
cat("ASV map created and saved to:", file.path(outdir, "ASV_sequence_to_ID_map.csv"), "\n")
# --- 3. Create New Count Table (ASV IDs) ---
# Transpose the original sequence table (ASV sequences as row names, Samples as column names)
ASV_counts_orig <- t(seqtab_all_nochim)
# Create the new count table
ASV_counts_new <- ASV_counts_orig
# Replace the row names (ASV sequences) with the new ASV IDs using the map
match_indices <- match(rownames(ASV_counts_new), ASV_map$Sequence)
rownames(ASV_counts_new) <- ASV_map$ASV_ID[match_indices]
# Write the new ASV count table
write.csv(ASV_counts_new, file.path(outdir, "ASV_counts_ID.csv"), quote = FALSE)
cat("ASV count table with IDs saved to:", file.path(outdir, "ASV_counts_ID.csv"), "\n")
# --- 4. Create New Taxonomy Table (ASV IDs) ---
# The taxonomy table 'taxa' has ASV sequences as row names
taxa_new <- taxa
# Replace the row names (ASV sequences) with the new ASV IDs
match_indices_taxa <- match(rownames(taxa_new), ASV_map$Sequence)
rownames(taxa_new) <- ASV_map$ASV_ID[match_indices_taxa]
# Write the new taxonomy table
write.csv(data.frame(taxa_new, check.names = FALSE), file.path(outdir, "ASV_table_taxonomy_ID.csv"), quote = FALSE)
cat("ASV taxonomy table with IDs saved to:", file.path(outdir, "ASV_table_taxonomy_ID.csv"), "\n")

############# Create a bacterial phylogenetic tree (Computationally Intensive) #############
# Create dna_sequences from ASV_map
# ASV sequences are the values, ASV IDs are the names
dna_sequences <- Biostrings::DNAStringSet(setNames(ASV_map$Sequence, ASV_map$ASV_ID))

library(DECIPHER)
library(phangorn)
library(Biostrings)
library(ape) # For final write.tree

# Align seqs:
alignment <- DECIPHER::AlignSeqs(dna_sequences, anchor = NA)
# Transform to phyDat:
phang.align <- phyDat(as.matrix(alignment), type = "DNA")
# Compute a distance matrix and Neighbor-Joining tree:
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm)
# Optimize with maximum likelihood:
fit <- pml(treeNJ, data = phang.align)
# Optimize using GTR model (Heavy computation):
fitGTR <- optim.pml(fit, model = "GTR", optInv = TRUE, optGamma = TRUE, rearrangement = "stochastic", control = pml.control(trace = 0))
# Extract final tree
final_tree <- fitGTR$tree
# 1. Save the final tree as a native R object (.rds)
saveRDS(final_tree, file.path(outdir, "asv_tree.rds"))
cat("Phylogenetic tree saved as RDS:", file.path(outdir, "asv_tree.rds"), "\n")
# 2. Save the final tree in the standard Newick format (.nwk)
write.tree(final_tree, file.path(outdir, "asv_tree.nwk"))
cat("Phylogenetic tree saved as Newick:", file.path(outdir, "asv_tree.nwk"), "\n")
